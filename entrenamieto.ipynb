{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705eeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # 1. Definir dÃ³nde queremos los datos (Root del proyecto)\n",
    "# TARGET_DIR = \"./RAVDESS_DATA\"\n",
    "\n",
    "# # 2. Descargar desde Kaggle (Se baja a una carpeta temporal de cachÃ©)\n",
    "# print(\"â¬‡ï¸ Iniciando descarga desde KaggleHub...\")\n",
    "# # Esta funciÃ³n devuelve la ruta donde Kaggle guardÃ³ los archivos temporalmente\n",
    "# cache_path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "\n",
    "# print(f\"âœ… Descarga completada en cachÃ©: {cache_path}\")\n",
    "\n",
    "# # 3. Mover archivos a tu carpeta del proyecto\n",
    "# print(f\"ðŸšš Moviendo archivos a '{TARGET_DIR}'...\")\n",
    "\n",
    "# # Crear tu carpeta si no existe\n",
    "# os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# # Mover el contenido (Carpetas Actor_01, Actor_02, etc.)\n",
    "# for item in os.listdir(cache_path):\n",
    "#     source = os.path.join(cache_path, item)\n",
    "#     destination = os.path.join(TARGET_DIR, item)\n",
    "    \n",
    "#     # Si ya existe la carpeta Actor_XX, la borramos para actualizarla\n",
    "#     if os.path.exists(destination):\n",
    "#         if os.path.isdir(destination):\n",
    "#             shutil.rmtree(destination)\n",
    "#         else:\n",
    "#             os.remove(destination)\n",
    "    \n",
    "#     # Mover (Move es mÃ¡s rÃ¡pido que Copy)\n",
    "#     shutil.move(source, destination)\n",
    "\n",
    "# print(\"------------------------------------------------\")\n",
    "# print(f\"ðŸŽ‰ Â¡Ã‰XITO! El dataset estÃ¡ listo en: {TARGET_DIR}\")\n",
    "# print(\"   Estructura verificada:\")\n",
    "# # Listar las primeras 5 carpetas para confirmar\n",
    "# print(f\"   {sorted(os.listdir(TARGET_DIR))[:5]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f54c79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ MODO TURBO: Usando NVIDIA GeForce RTX 2080 Ti\n",
      "ðŸ§  VRAM Disponible: 11.36 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import platform\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor\n",
    "# Importamos la barra de progreso para Notebooks\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURACIÃ“N GLOBAL ---\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# DetecciÃ³n de Hardware\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\") # Usamos la primera GPU\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    # Mostramos info de la GPU\n",
    "    print(f\"ðŸš€ MODO TURBO: Usando {gpu_name}\")\n",
    "    print(f\"ðŸ§  VRAM Disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ Usando CPU.\")\n",
    "\n",
    "# La cuantizaciÃ³n final siempre es en CPU\n",
    "QUANT_DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff91ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RavdessResearchDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, duration=3.0, add_noise=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.num_samples = int(sample_rate * duration)\n",
    "        self.add_noise = add_noise\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.actors = [] \n",
    "        \n",
    "        print(f\"ðŸ“‚ Escaneando directorio: {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.wav', '.flac', '.mp3')):\n",
    "                    parts = file.split('-')\n",
    "                    if len(parts) == 7:\n",
    "                        # RAVDESS: 03-01-03-01-01-01-01.wav -> Emotion is index 2\n",
    "                        emotion = int(parts[2]) - 1 \n",
    "                        actor = int(parts[6].split('.')[0])\n",
    "                        self.labels.append(emotion)\n",
    "                        self.actors.append(actor)\n",
    "                        self.file_paths.append(os.path.join(root, file))\n",
    "        \n",
    "        print(f\"âœ… Total archivos encontrados: {len(self.file_paths)}\")\n",
    "\n",
    "        # Transformaciones de Audio\n",
    "        self.mel_spec = MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=512, n_mels=64)\n",
    "        self.db_transform = AmplitudeToDB()\n",
    "\n",
    "    def _process_signal(self, path):\n",
    "        try:\n",
    "            data, sr = sf.read(path)\n",
    "        except:\n",
    "            return torch.zeros(self.num_samples) # Retorno seguro si falla\n",
    "\n",
    "        if len(data.shape) > 1: data = np.mean(data, axis=1) # Stereo a Mono\n",
    "        if sr != self.sample_rate: \n",
    "            data = resampy.resample(data, sr, self.sample_rate, filter='kaiser_best')\n",
    "        \n",
    "        signal = torch.from_numpy(data).float()\n",
    "        \n",
    "        # Cortar o Rellenar (Padding) a 3 segundos\n",
    "        if signal.shape[0] > self.num_samples:\n",
    "            signal = signal[:self.num_samples]\n",
    "        elif signal.shape[0] < self.num_samples:\n",
    "            pad = self.num_samples - signal.shape[0]\n",
    "            signal = F.pad(signal, (0, pad))\n",
    "            \n",
    "        return signal\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            signal = self._process_signal(path)\n",
    "            \n",
    "            # Data Augmentation (Solo ruido)\n",
    "            if self.add_noise:\n",
    "                noise = torch.randn_like(signal) * 0.005\n",
    "                signal = signal + noise\n",
    "            \n",
    "            # Input 1: Espectrograma (Para tu modelo Alumno)\n",
    "            spec = self.mel_spec(signal)\n",
    "            spec = self.db_transform(spec)\n",
    "            \n",
    "            # Input 2: Audio Crudo (Para el modelo Maestro Wav2Vec)\n",
    "            return {\n",
    "                \"spectrogram\": spec.unsqueeze(0), # [1, 64, 94]\n",
    "                \"raw_audio\": signal,              # [48000]\n",
    "                \"label\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except:\n",
    "            return self.__getitem__((idx + 1) % len(self)) # Intentar siguiente si falla\n",
    "    \n",
    "    def __len__(self): return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda21a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Escaneando directorio: ./RAVDESS_DATA\n",
      "âœ… Total archivos encontrados: 2880\n",
      "ðŸ“‚ Escaneando directorio: ./RAVDESS_DATA\n",
      "âœ… Total archivos encontrados: 2880\n",
      "ðŸ“‚ Escaneando directorio: ./RAVDESS_DATA\n",
      "âœ… Total archivos encontrados: 2880\n",
      "ðŸ“Š Train: 2400 | Val: 480\n",
      "âœ… DataLoaders listos (Batch Size: 64, Workers: 4).\n"
     ]
    }
   ],
   "source": [
    "# --- RUTA DE TUS DATOS ---\n",
    "ROOT_PATH = \"./RAVDESS_DATA\" \n",
    "\n",
    "# 1. Instanciar Dataset\n",
    "full_dataset = RavdessResearchDataset(root_dir=ROOT_PATH, add_noise=False)\n",
    "\n",
    "if len(full_dataset) == 0:\n",
    "    print(\"âŒ ERROR CRÃTICO: No se encontraron archivos.\")\n",
    "else:\n",
    "    # 2. Split por Actor\n",
    "    all_actors = full_dataset.actors\n",
    "    train_indices = [i for i, a in enumerate(all_actors) if a <= 20]\n",
    "    val_indices = [i for i, a in enumerate(all_actors) if a > 20]\n",
    "\n",
    "    train_ds = Subset(RavdessResearchDataset(ROOT_PATH, add_noise=True), train_indices)\n",
    "    val_ds = Subset(RavdessResearchDataset(ROOT_PATH, add_noise=False), val_indices)\n",
    "\n",
    "    print(f\"ðŸ“Š Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "\n",
    "    # 3. DATALOADERS OPTIMIZADOS\n",
    "    # Batch size 64 para aprovechar tu VRAM\n",
    "    # num_workers=4 para que la CPU prepare los datos rÃ¡pido mientras la GPU entrena\n",
    "    # pin_memory=True acelera el paso de RAM a VRAM\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(\"âœ… DataLoaders listos (Batch Size: 64, Workers: 4).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5c66ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- CORRECCIÃ“N FINAL: SOPORTE NATIVO DE KERNEL SIZE ---\n",
    "\n",
    "# Bloque 1: RepVGG Standard (Ahora soporta kernel_size variable)\n",
    "class RepVGGBlock(nn.Module):\n",
    "    # AÃ‘ADIDO: argumento 'kernel_size' al init\n",
    "    def __init__(self, in_c, out_c, stride=1, deploy=False, groups=1, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        if deploy:\n",
    "            self.rbr_reparam = nn.Conv2d(in_c, out_c, kernel_size, stride, padding, groups=groups, bias=True)\n",
    "        else:\n",
    "            # Rama Densa: Usa el kernel_size que le pasemos (3 para DW, 1 para PW)\n",
    "            self.rbr_dense = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, kernel_size, stride, padding, groups=groups, bias=False), \n",
    "                nn.BatchNorm2d(out_c)\n",
    "            )\n",
    "            \n",
    "            # Rama 1x1: Solo se aÃ±ade si el kernel principal NO es 1x1 (para evitar redundancia total)\n",
    "            # o si queremos forzar diversidad. En MobileOne standard:\n",
    "            # - DW (3x3): Tiene rama 1x1.\n",
    "            # - PW (1x1): Es puramente densa 1x1 (la rama 1x1 serÃ­a idÃ©ntica a la densa).\n",
    "            # Para simplificar y asegurar estabilidad:\n",
    "            if kernel_size > 1:\n",
    "                padding_1x1 = 0\n",
    "                self.rbr_1x1 = nn.Sequential(\n",
    "                    nn.Conv2d(in_c, out_c, 1, stride, padding_1x1, groups=groups, bias=False), \n",
    "                    nn.BatchNorm2d(out_c)\n",
    "                )\n",
    "            else:\n",
    "                self.rbr_1x1 = None # No rama 1x1 extra si la principal ya es 1x1\n",
    "\n",
    "            # Rama Identidad: Solo si in==out y stride==1\n",
    "            self.rbr_identity = nn.BatchNorm2d(in_c) if out_c == in_c and stride == 1 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deploy: return self.act(self.rbr_reparam(x))\n",
    "        \n",
    "        # Salida de rama densa\n",
    "        out = self.rbr_dense(x)\n",
    "        \n",
    "        # Sumamos rama 1x1 si existe\n",
    "        if self.rbr_1x1 is not None:\n",
    "            out += self.rbr_1x1(x)\n",
    "            \n",
    "        # Sumamos identidad si existe\n",
    "        if self.rbr_identity is not None:\n",
    "            out += self.rbr_identity(x)\n",
    "            \n",
    "        return self.act(out)\n",
    "\n",
    "    def switch_to_deploy(self):\n",
    "        if self.deploy: return\n",
    "        \n",
    "        # Obtenemos configuraciÃ³n de la rama densa actual\n",
    "        dense_conv = self.rbr_dense[0]\n",
    "        k = dense_conv.kernel_size[0]\n",
    "        s = dense_conv.stride[0]\n",
    "        p = dense_conv.padding[0]\n",
    "        g = dense_conv.groups\n",
    "        \n",
    "        self.rbr_reparam = nn.Conv2d(\n",
    "            dense_conv.in_channels, \n",
    "            dense_conv.out_channels, \n",
    "            k, s, p, groups=g, bias=True\n",
    "        )\n",
    "        # Copia simple de pesos (en producciÃ³n real se fusionarÃ­an las ramas)\n",
    "        self.rbr_reparam.weight.data = dense_conv.weight.data\n",
    "        self.rbr_reparam.bias.data = torch.zeros(self.rbr_reparam.out_channels)\n",
    "        \n",
    "        self.deploy = True\n",
    "\n",
    "# Bloque 2: MobileOne (Actualizado para usar el nuevo RepVGGBlock)\n",
    "class MobileOneBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1, deploy=False):\n",
    "        super().__init__()\n",
    "        # Depthwise: 3x3, Padding 1, Grupos=in_c\n",
    "        self.dw = RepVGGBlock(in_c, in_c, stride, deploy=deploy, groups=in_c, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pointwise: 1x1, Padding 0, Grupos=1\n",
    "        # AQUÃ ESTABA EL ERROR: Ahora pasamos kernel_size=1 explÃ­citamente\n",
    "        self.pw = RepVGGBlock(in_c, out_c, 1, deploy=deploy, groups=1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        return x\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        self.dw.switch_to_deploy()\n",
    "        self.pw.switch_to_deploy()\n",
    "\n",
    "# Bloque 3: GhostModule (Sin cambios)\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1):\n",
    "        super().__init__()\n",
    "        self.oup = oup\n",
    "        init_channels = int(oup / 2)\n",
    "        new_channels = init_channels\n",
    "        \n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv2d(inp, init_channels, 1, stride, 0, bias=False),\n",
    "            nn.BatchNorm2d(init_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv2d(init_channels, new_channels, 3, 1, 1, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm2d(new_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "        return torch.cat([x1, x2], dim=1)[:, :self.oup, :, :]\n",
    "\n",
    "# RED PRINCIPAL\n",
    "class UniversalAudioNet(nn.Module):\n",
    "    def __init__(self, num_classes=8, arch_type=\"repvgg\", deploy=False):\n",
    "        super().__init__()\n",
    "        self.arch_type = arch_type\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        def make_block(in_c, out_c, stride):\n",
    "            if arch_type == \"repvgg\": \n",
    "                # RepVGG normal usa 3x3\n",
    "                return RepVGGBlock(in_c, out_c, stride, deploy=deploy, kernel_size=3, padding=1)\n",
    "            elif arch_type == \"mobileone\": \n",
    "                return MobileOneBlock(in_c, out_c, stride, deploy=deploy)\n",
    "            elif arch_type == \"ghostnet\": \n",
    "                return GhostModule(in_c, out_c, stride)\n",
    "\n",
    "        # Backbone Tiny\n",
    "        self.stage0 = make_block(1, 32, 2)\n",
    "        self.stage1 = make_block(32, 64, 2)\n",
    "        self.stage2 = make_block(64, 128, 2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.stage0(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        x = self.fc(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        for m in self.modules():\n",
    "            if m is self: continue\n",
    "            if hasattr(m, 'switch_to_deploy'): m.switch_to_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d900614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¨â€ðŸ« Cargando Maestro: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/audio_edge_env/lib/python3.10/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Maestro cargado en GPU.\n"
     ]
    }
   ],
   "source": [
    "TEACHER_MODEL_NAME = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "\n",
    "try:\n",
    "    print(f\"ðŸ‘¨â€ðŸ« Cargando Maestro: {TEACHER_MODEL_NAME}...\")\n",
    "    teacher_model = AutoModelForAudioClassification.from_pretrained(TEACHER_MODEL_NAME)\n",
    "    teacher_model.to(DEVICE)\n",
    "    teacher_model.eval()\n",
    "    TEACHER_AVAILABLE = True\n",
    "    print(\"âœ… Maestro cargado en GPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error cargando maestro: {e}\")\n",
    "    TEACHER_AVAILABLE = False\n",
    "    teacher_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e697231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 Corregida\n",
    "\n",
    "def run_experiment(arch_name, train_loader, val_loader, epochs=10):\n",
    "    print(f\"\\nðŸ”¬ --- ARQUITECTURA: {arch_name.upper()} ---\")\n",
    "    \n",
    "    model = UniversalAudioNet(arch_type=arch_name).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 1. CICLO DE ENTRENAMIENTO\n",
    "    # Quitamos 'position' para evitar saltos de lÃ­nea\n",
    "    epoch_bar = tqdm(range(epochs), desc=f\"Entrenando {arch_name}\", unit=\"epoch\", leave=True)\n",
    "    \n",
    "    # Historial\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Barra interna para los batches\n",
    "        # leave=False borra esta barra al terminar la Ã©poca para no ensuciar\n",
    "        batch_bar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False, unit=\"batch\")\n",
    "        \n",
    "        for batch in batch_bar:\n",
    "            specs = batch['spectrogram'].to(DEVICE)\n",
    "            raw = batch['raw_audio'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            student_logits = model(specs)\n",
    "            loss = criterion(student_logits, labels)\n",
    "            \n",
    "            # DestilaciÃ³n\n",
    "            if TEACHER_AVAILABLE and teacher_model:\n",
    "                with torch.no_grad():\n",
    "                    t_out = teacher_model(raw)\n",
    "                    t_logits = t_out.logits if hasattr(t_out, 'logits') else t_out\n",
    "                \n",
    "                if t_logits.shape[1] == student_logits.shape[1]:\n",
    "                    T = 4.0\n",
    "                    d_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "                        F.log_softmax(student_logits/T, dim=1),\n",
    "                        F.softmax(t_logits/T, dim=1)\n",
    "                    ) * (T*T)\n",
    "                    loss = 0.5 * loss + 0.5 * d_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            # Actualizamos la barra pequeÃ±a\n",
    "            batch_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            \n",
    "        # 2. EVALUACIÃ“N\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                out = model(batch['spectrogram'].to(DEVICE))\n",
    "                _, pred = torch.max(out, 1)\n",
    "                correct += (pred == batch['label'].to(DEVICE)).sum().item()\n",
    "                total += batch['label'].size(0)\n",
    "        \n",
    "        val_acc = 100 * correct / total\n",
    "        history['train_loss'].append(running_loss / len(train_loader))\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Actualizamos la barra PRINCIPAL\n",
    "        epoch_bar.set_postfix(val_acc=f\"{val_acc:.2f}%\", avg_loss=f\"{running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # 3. OPTIMIZACIÃ“N FINAL (CPU)\n",
    "    print(\"\\nðŸ§Š Optimizando (Poda + CuantizaciÃ³n)...\")\n",
    "    model.to(\"cpu\")\n",
    "    if hasattr(model, 'switch_to_deploy'): model.switch_to_deploy()\n",
    "        \n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            try:\n",
    "                prune.l1_unstructured(m, name='weight', amount=0.3)\n",
    "                prune.remove(m, 'weight')\n",
    "            except: pass\n",
    "            \n",
    "    # CuantizaciÃ³n\n",
    "    backend = 'qnnpack' if 'arm' in platform.machine().lower() else 'fbgemm'\n",
    "    model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "    torch.backends.quantized.engine = backend\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    \n",
    "    # CalibraciÃ³n rÃ¡pida\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            if i > 5: break\n",
    "            model(batch['spectrogram'].to(\"cpu\"))\n",
    "    \n",
    "    model_int8 = torch.quantization.convert(model, inplace=False)\n",
    "    \n",
    "    # EvaluaciÃ³n Final INT8\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model_int8(batch['spectrogram'].to(\"cpu\"))\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct += (pred == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "    acc_int8 = 100 * correct / total\n",
    "    \n",
    "    # TamaÃ±o\n",
    "    torch.save(model_int8.state_dict(), f\"temp_{arch_name}.pth\")\n",
    "    size_mb = os.path.getsize(f\"temp_{arch_name}.pth\") / (1024*1024)\n",
    "    \n",
    "    print(f\"âœ… FINAL {arch_name}: Acc={acc_int8:.2f}% | Size={size_mb:.2f} MB\")\n",
    "    return {\"arch\": arch_name, \"acc_int8\": acc_int8, \"size\": size_mb, \"history\": history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ba6604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¬ --- ARQUITECTURA: REPVGG ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando repvgg:   0%|          | 0/3 [00:00<?, ?epoch/s]\n",
      "Ep 1:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 1:   0%|          | 0/38 [00:03<?, ?batch/s, loss=1.0791]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:03<02:19,  3.78s/batch, loss=1.0791]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:04<02:19,  3.78s/batch, loss=1.0084]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:04<01:09,  1.93s/batch, loss=1.0084]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:05<01:09,  1.93s/batch, loss=1.0148]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=1.0148]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=0.9899]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:05<00:36,  1.07s/batch, loss=0.9899]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:06<00:36,  1.07s/batch, loss=0.9630]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9630]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:07<00:30,  1.09batch/s, loss=0.9868]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=0.9868]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=0.9995]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.30batch/s, loss=0.9995]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.30batch/s, loss=0.9834]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:21,  1.37batch/s, loss=0.9834]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:21,  1.37batch/s, loss=0.9595]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.42batch/s, loss=0.9595]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.42batch/s, loss=0.9745]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.46batch/s, loss=0.9745]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.46batch/s, loss=0.9858]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.48batch/s, loss=0.9858]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.48batch/s, loss=0.9428]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.50batch/s, loss=0.9428]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.50batch/s, loss=0.9496]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.51batch/s, loss=0.9496]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.51batch/s, loss=0.9443]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.52batch/s, loss=0.9443]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.52batch/s, loss=0.9529]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.53batch/s, loss=0.9529]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.53batch/s, loss=0.9958]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.53batch/s, loss=0.9958]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.53batch/s, loss=0.9398]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.53batch/s, loss=0.9398]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.53batch/s, loss=0.9811]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.54batch/s, loss=0.9811]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.54batch/s, loss=0.9828]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.54batch/s, loss=0.9828]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.54batch/s, loss=0.9737]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.54batch/s, loss=0.9737]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.54batch/s, loss=0.9453]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.54batch/s, loss=0.9453]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.54batch/s, loss=0.9394]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.54batch/s, loss=0.9394]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.54batch/s, loss=0.9727]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.54batch/s, loss=0.9727]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.54batch/s, loss=0.9602]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.54batch/s, loss=0.9602]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.54batch/s, loss=0.9684]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.54batch/s, loss=0.9684]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.54batch/s, loss=1.0296]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:19<00:07,  1.54batch/s, loss=1.0296]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.54batch/s, loss=0.9772]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.54batch/s, loss=0.9772]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.54batch/s, loss=0.9489]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.54batch/s, loss=0.9489]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.54batch/s, loss=0.9452]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:21<00:05,  1.54batch/s, loss=0.9452]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.54batch/s, loss=0.9768]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.54batch/s, loss=0.9768]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.54batch/s, loss=0.9680]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.53batch/s, loss=0.9680]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.53batch/s, loss=0.9872]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:23<00:03,  1.53batch/s, loss=0.9872]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.53batch/s, loss=1.0038]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.53batch/s, loss=1.0038]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.53batch/s, loss=0.9663]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.53batch/s, loss=0.9663]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.53batch/s, loss=0.9704]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:25<00:01,  1.53batch/s, loss=0.9704]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.53batch/s, loss=0.9615]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.53batch/s, loss=0.9615]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.53batch/s, loss=0.9624]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.53batch/s, loss=0.9624]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.53batch/s, loss=0.9411]\u001b[A\n",
      "Ep 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.79batch/s, loss=0.9411]\u001b[A\n",
      "Entrenando repvgg:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:32<01:05, 32.92s/epoch, avg_loss=0.9745, val_acc=13.33%]\n",
      "Ep 2:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 2:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9556]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:03<02:12,  3.58s/batch, loss=0.9556]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:04<02:12,  3.58s/batch, loss=0.9319]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:04<01:06,  1.86s/batch, loss=0.9319]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:04<01:06,  1.86s/batch, loss=0.9424]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:04<00:45,  1.31s/batch, loss=0.9424]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:05<00:45,  1.31s/batch, loss=0.9403]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:05<00:35,  1.05s/batch, loss=0.9403]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:06<00:35,  1.05s/batch, loss=0.9547]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:29,  1.10batch/s, loss=0.9547]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:29,  1.10batch/s, loss=0.9503]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.22batch/s, loss=0.9503]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.22batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.31batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.31batch/s, loss=0.9572]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:21,  1.37batch/s, loss=0.9572]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:21,  1.37batch/s, loss=0.9785]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.42batch/s, loss=0.9785]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.42batch/s, loss=0.9736]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.45batch/s, loss=0.9736]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.45batch/s, loss=0.9656]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.47batch/s, loss=0.9656]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.47batch/s, loss=0.9294]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.49batch/s, loss=0.9294]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.49batch/s, loss=0.9879]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.50batch/s, loss=0.9879]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.50batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.51batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.51batch/s, loss=0.9419]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.52batch/s, loss=0.9419]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.52batch/s, loss=0.9233]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.52batch/s, loss=0.9233]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.52batch/s, loss=0.9601]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.52batch/s, loss=0.9601]\u001b[A\n",
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.52batch/s, loss=0.9382]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.52batch/s, loss=0.9382]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.52batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.52batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.52batch/s, loss=0.9473]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:15<00:11,  1.52batch/s, loss=0.9473]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.52batch/s, loss=0.9167]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.52batch/s, loss=0.9167]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.52batch/s, loss=0.9114]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.53batch/s, loss=0.9114]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.53batch/s, loss=0.8867]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:17<00:09,  1.53batch/s, loss=0.8867]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.53batch/s, loss=0.8708]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.52batch/s, loss=0.8708]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.52batch/s, loss=0.9521]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.52batch/s, loss=0.9521]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.52batch/s, loss=0.9218]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:19<00:07,  1.52batch/s, loss=0.9218]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.52batch/s, loss=0.9102]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.52batch/s, loss=0.9102]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.52batch/s, loss=0.9228]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.52batch/s, loss=0.9228]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.52batch/s, loss=0.9362]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:21<00:05,  1.52batch/s, loss=0.9362]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.52batch/s, loss=0.9341]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.52batch/s, loss=0.9341]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.52batch/s, loss=0.9063]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.52batch/s, loss=0.9063]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.52batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:23<00:03,  1.52batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.52batch/s, loss=0.9283]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.52batch/s, loss=0.9283]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.52batch/s, loss=0.9143]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.52batch/s, loss=0.9143]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.52batch/s, loss=0.9303]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:25<00:01,  1.52batch/s, loss=0.9303]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.52batch/s, loss=0.9052]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.52batch/s, loss=0.9052]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.52batch/s, loss=0.9214]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.52batch/s, loss=0.9214]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.52batch/s, loss=0.9065]\u001b[A\n",
      "Ep 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.78batch/s, loss=0.9065]\u001b[A\n",
      "Entrenando repvgg:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:05<00:32, 32.93s/epoch, avg_loss=0.9356, val_acc=13.33%]\n",
      "Ep 3:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 3:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9345]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:03<02:14,  3.62s/batch, loss=0.9345]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:04<02:14,  3.62s/batch, loss=0.9153]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:07,  1.88s/batch, loss=0.9153]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:07,  1.88s/batch, loss=0.9262]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:04<00:46,  1.32s/batch, loss=0.9262]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:05<00:46,  1.32s/batch, loss=0.8974]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:05<00:35,  1.06s/batch, loss=0.8974]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:06<00:35,  1.06s/batch, loss=0.9217]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9217]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9034]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.21batch/s, loss=0.9034]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=0.9096]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.30batch/s, loss=0.9096]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.30batch/s, loss=0.9248]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=0.9248]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=0.9279]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.41batch/s, loss=0.9279]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.41batch/s, loss=0.8892]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.44batch/s, loss=0.8892]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.44batch/s, loss=0.9107]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9107]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.8865]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.48batch/s, loss=0.8865]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.48batch/s, loss=0.9149]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.49batch/s, loss=0.9149]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.49batch/s, loss=0.9358]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.50batch/s, loss=0.9358]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.50batch/s, loss=0.9328]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=0.9328]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.8956]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.51batch/s, loss=0.8956]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.51batch/s, loss=0.8893]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.8893]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9177]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=0.9177]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=0.8840]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=0.8840]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=0.9027]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9027]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9277]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=0.9277]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=0.8980]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.52batch/s, loss=0.8980]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.52batch/s, loss=0.9001]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9001]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.8749]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.8749]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9489]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.52batch/s, loss=0.9489]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.52batch/s, loss=0.9002]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9002]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.8954]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.8954]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.8787]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.8787]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.8929]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.8929]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9100]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9100]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.8968]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.8968]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9135]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9135]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9390]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9390]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9164]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.51batch/s, loss=0.9164]\u001b[A\n",
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.51batch/s, loss=0.9267]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9267]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9014]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=0.9014]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=0.8921]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.8921]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.8821]\u001b[A\n",
      "Ep 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.77batch/s, loss=0.8821]\u001b[A\n",
      "Entrenando repvgg: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:38<00:00, 32.98s/epoch, avg_loss=0.9083, val_acc=13.75%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§Š Optimizando (Poda + CuantizaciÃ³n)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_773191/4175620692.py:88: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  torch.quantization.prepare(model, inplace=True)\n",
      "/root/miniconda3/envs/audio_edge_env/lib/python3.10/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_773191/4175620692.py:96: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_int8 = torch.quantization.convert(model, inplace=False)\n",
      "/root/miniconda3/envs/audio_edge_env/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1343: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL repvgg: Acc=13.33% | Size=0.23 MB\n",
      "\n",
      "ðŸ”¬ --- ARQUITECTURA: MOBILEONE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando mobileone:   0%|          | 0/3 [00:00<?, ?epoch/s]\n",
      "Ep 1:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 1:   0%|          | 0/38 [00:03<?, ?batch/s, loss=1.0442]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:03<02:19,  3.77s/batch, loss=1.0442]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:04<02:19,  3.77s/batch, loss=1.0400]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:04<01:09,  1.94s/batch, loss=1.0400]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:05<01:09,  1.94s/batch, loss=1.0239]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=1.0239]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=1.0366]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:05<00:36,  1.08s/batch, loss=1.0366]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:06<00:36,  1.08s/batch, loss=1.0224]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.08batch/s, loss=1.0224]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:07<00:30,  1.08batch/s, loss=1.0322]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.20batch/s, loss=1.0322]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.20batch/s, loss=1.0154]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:07<00:24,  1.29batch/s, loss=1.0154]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:08<00:24,  1.29batch/s, loss=1.0257]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=1.0257]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:09<00:22,  1.36batch/s, loss=1.0130]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=1.0130]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=1.0108]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.44batch/s, loss=1.0108]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.44batch/s, loss=1.0396]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=1.0396]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=1.0412]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.48batch/s, loss=1.0412]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.48batch/s, loss=1.0245]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.49batch/s, loss=1.0245]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.49batch/s, loss=1.0274]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.50batch/s, loss=1.0274]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:15,  1.50batch/s, loss=1.0260]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.51batch/s, loss=1.0260]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.51batch/s, loss=1.0119]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.51batch/s, loss=1.0119]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.51batch/s, loss=1.0049]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=1.0049]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=1.0025]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=1.0025]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=1.0085]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=1.0085]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=1.0266]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.52batch/s, loss=1.0266]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.52batch/s, loss=1.0129]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.52batch/s, loss=1.0129]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.52batch/s, loss=1.0081]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.52batch/s, loss=1.0081]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.52batch/s, loss=1.0018]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.52batch/s, loss=1.0018]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.52batch/s, loss=0.9892]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.52batch/s, loss=0.9892]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.52batch/s, loss=1.0120]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=1.0120]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=1.0003]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=1.0003]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9924]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9924]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9901]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9901]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9881]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9881]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9985]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9985]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9951]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9951]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9858]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9858]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9693]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.52batch/s, loss=0.9693]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.52batch/s, loss=0.9907]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.52batch/s, loss=0.9907]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.52batch/s, loss=0.9943]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9943]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9855]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=0.9855]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=1.0152]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=1.0152]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9795]\u001b[A\n",
      "Ep 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.77batch/s, loss=0.9795]\u001b[A\n",
      "Entrenando mobileone:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:33<01:06, 33.44s/epoch, avg_loss=1.0102, val_acc=13.33%]\n",
      "Ep 2:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 2:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9763]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:03<02:18,  3.73s/batch, loss=0.9763]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:04<02:18,  3.73s/batch, loss=1.0070]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:04<01:09,  1.92s/batch, loss=1.0070]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:05<01:09,  1.92s/batch, loss=0.9828]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=0.9828]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:05<00:47,  1.35s/batch, loss=0.9751]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:05<00:36,  1.08s/batch, loss=0.9751]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:06<00:36,  1.08s/batch, loss=0.9429]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.08batch/s, loss=0.9429]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:07<00:30,  1.08batch/s, loss=0.9901]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.20batch/s, loss=0.9901]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.20batch/s, loss=0.9938]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:07<00:24,  1.29batch/s, loss=0.9938]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:08<00:24,  1.29batch/s, loss=0.9841]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.35batch/s, loss=0.9841]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:09<00:22,  1.35batch/s, loss=0.9720]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=0.9720]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=0.9976]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.43batch/s, loss=0.9976]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.43batch/s, loss=0.9766]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9766]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9842]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.47batch/s, loss=0.9842]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.47batch/s, loss=0.9954]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.49batch/s, loss=0.9954]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.49batch/s, loss=0.9669]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9669]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9920]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=0.9920]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.9795]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.50batch/s, loss=0.9795]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.50batch/s, loss=0.9868]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9868]\u001b[A\n",
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9728]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=0.9728]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=0.9815]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=0.9815]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=0.9998]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9998]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9674]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=0.9674]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=0.9990]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.51batch/s, loss=0.9990]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.51batch/s, loss=0.9739]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9739]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9581]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.9581]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9642]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=0.9642]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=0.9811]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9811]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9596]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9596]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9578]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9578]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9638]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9638]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9700]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9700]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9902]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9902]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9563]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9563]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9467]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9467]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9385]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.51batch/s, loss=0.9385]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.51batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9502]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9637]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=0.9637]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=0.9926]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9926]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9744]\u001b[A\n",
      "Ep 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.76batch/s, loss=0.9744]\u001b[A\n",
      "Entrenando mobileone:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:06<00:33, 33.41s/epoch, avg_loss=0.9754, val_acc=13.33%]\n",
      "Ep 3:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 3:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9702]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:03<02:15,  3.67s/batch, loss=0.9702]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:04<02:15,  3.67s/batch, loss=0.9657]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:08,  1.90s/batch, loss=0.9657]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:08,  1.90s/batch, loss=0.9516]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:04<00:46,  1.33s/batch, loss=0.9516]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:05<00:46,  1.33s/batch, loss=0.9664]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:05<00:36,  1.07s/batch, loss=0.9664]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:06<00:36,  1.07s/batch, loss=0.9751]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9751]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9517]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.20batch/s, loss=0.9517]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.20batch/s, loss=0.9620]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:07<00:24,  1.29batch/s, loss=0.9620]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:08<00:24,  1.29batch/s, loss=0.9617]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.35batch/s, loss=0.9617]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.35batch/s, loss=0.9477]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.40batch/s, loss=0.9477]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=0.9574]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.43batch/s, loss=0.9574]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.43batch/s, loss=0.9529]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.45batch/s, loss=0.9529]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.45batch/s, loss=0.9796]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.47batch/s, loss=0.9796]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.47batch/s, loss=0.9631]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.48batch/s, loss=0.9631]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.48batch/s, loss=0.9775]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9775]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9803]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=0.9803]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.9544]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.50batch/s, loss=0.9544]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.50batch/s, loss=0.9691]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.50batch/s, loss=0.9691]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.50batch/s, loss=0.9765]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=0.9765]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=0.9760]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=0.9760]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=0.9434]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9434]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9653]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=0.9653]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=0.9474]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.51batch/s, loss=0.9474]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.51batch/s, loss=0.9697]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9697]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9414]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.9414]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9609]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=0.9609]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=0.9620]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9620]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9386]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9386]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9555]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9555]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9762]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9762]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9685]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9685]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9865]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9865]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9521]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9521]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9568]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9568]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9547]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.51batch/s, loss=0.9547]\u001b[A\n",
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.51batch/s, loss=1.0029]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=1.0029]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9572]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=0.9572]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=0.9479]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9479]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9913]\u001b[A\n",
      "Ep 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.76batch/s, loss=0.9913]\u001b[A\n",
      "Entrenando mobileone: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:40<00:00, 33.40s/epoch, avg_loss=0.9636, val_acc=13.33%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§Š Optimizando (Poda + CuantizaciÃ³n)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL mobileone: Acc=13.33% | Size=0.09 MB\n",
      "\n",
      "ðŸ”¬ --- ARQUITECTURA: GHOSTNET ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando ghostnet:   0%|          | 0/3 [00:00<?, ?epoch/s]\n",
      "Ep 1:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 1:   0%|          | 0/38 [00:03<?, ?batch/s, loss=1.0393]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:03<02:11,  3.56s/batch, loss=1.0393]\u001b[A\n",
      "Ep 1:   3%|â–Ž         | 1/38 [00:04<02:11,  3.56s/batch, loss=1.0313]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:04<01:06,  1.85s/batch, loss=1.0313]\u001b[A\n",
      "Ep 1:   5%|â–Œ         | 2/38 [00:04<01:06,  1.85s/batch, loss=1.0179]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:04<00:45,  1.31s/batch, loss=1.0179]\u001b[A\n",
      "Ep 1:   8%|â–Š         | 3/38 [00:05<00:45,  1.31s/batch, loss=1.0257]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:05<00:35,  1.05s/batch, loss=1.0257]\u001b[A\n",
      "Ep 1:  11%|â–ˆ         | 4/38 [00:06<00:35,  1.05s/batch, loss=1.0322]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.10batch/s, loss=1.0322]\u001b[A\n",
      "Ep 1:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.10batch/s, loss=1.0037]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.21batch/s, loss=1.0037]\u001b[A\n",
      "Ep 1:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=1.0147]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.30batch/s, loss=1.0147]\u001b[A\n",
      "Ep 1:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.30batch/s, loss=1.0077]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=1.0077]\u001b[A\n",
      "Ep 1:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=1.0066]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.41batch/s, loss=1.0066]\u001b[A\n",
      "Ep 1:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.41batch/s, loss=1.0044]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.44batch/s, loss=1.0044]\u001b[A\n",
      "Ep 1:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.44batch/s, loss=0.9990]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9990]\u001b[A\n",
      "Ep 1:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=1.0174]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.48batch/s, loss=1.0174]\u001b[A\n",
      "Ep 1:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.48batch/s, loss=0.9813]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.49batch/s, loss=0.9813]\u001b[A\n",
      "Ep 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.49batch/s, loss=0.9789]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.50batch/s, loss=0.9789]\u001b[A\n",
      "Ep 1:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.50batch/s, loss=1.0027]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=1.0027]\u001b[A\n",
      "Ep 1:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.9819]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.51batch/s, loss=0.9819]\u001b[A\n",
      "Ep 1:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.51batch/s, loss=0.9999]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9999]\u001b[A\n",
      "Ep 1:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9801]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=0.9801]\u001b[A\n",
      "Ep 1:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=1.0194]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=1.0194]\u001b[A\n",
      "Ep 1:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=1.0181]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=1.0181]\u001b[A\n",
      "Ep 1:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=1.0074]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=1.0074]\u001b[A\n",
      "Ep 1:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=1.0022]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.51batch/s, loss=1.0022]\u001b[A\n",
      "Ep 1:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.51batch/s, loss=0.9938]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9938]\u001b[A\n",
      "Ep 1:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9745]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.9745]\u001b[A\n",
      "Ep 1:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9784]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=0.9784]\u001b[A\n",
      "Ep 1:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=0.9911]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9911]\u001b[A\n",
      "Ep 1:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9562]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9562]\u001b[A\n",
      "Ep 1:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9705]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9705]\u001b[A\n",
      "Ep 1:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9904]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9904]\u001b[A\n",
      "Ep 1:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9677]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9677]\u001b[A\n",
      "Ep 1:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9754]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9754]\u001b[A\n",
      "Ep 1:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9719]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9719]\u001b[A\n",
      "Ep 1:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9783]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9783]\u001b[A\n",
      "Ep 1:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9794]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.51batch/s, loss=0.9794]\u001b[A\n",
      "Ep 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.51batch/s, loss=0.9847]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9847]\u001b[A\n",
      "Ep 1:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9746]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=0.9746]\u001b[A\n",
      "Ep 1:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=0.9756]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9756]\u001b[A\n",
      "Ep 1:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9852]\u001b[A\n",
      "Ep 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.76batch/s, loss=0.9852]\u001b[A\n",
      "Entrenando ghostnet:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:33<01:06, 33.18s/epoch, avg_loss=0.9952, val_acc=19.58%]\n",
      "Ep 2:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 2:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9983]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:03<02:13,  3.60s/batch, loss=0.9983]\u001b[A\n",
      "Ep 2:   3%|â–Ž         | 1/38 [00:04<02:13,  3.60s/batch, loss=0.9845]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:04<01:07,  1.87s/batch, loss=0.9845]\u001b[A\n",
      "Ep 2:   5%|â–Œ         | 2/38 [00:04<01:07,  1.87s/batch, loss=0.9705]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:04<00:46,  1.32s/batch, loss=0.9705]\u001b[A\n",
      "Ep 2:   8%|â–Š         | 3/38 [00:05<00:46,  1.32s/batch, loss=1.0039]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:05<00:36,  1.06s/batch, loss=1.0039]\u001b[A\n",
      "Ep 2:  11%|â–ˆ         | 4/38 [00:06<00:36,  1.06s/batch, loss=0.9526]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9526]\u001b[A\n",
      "Ep 2:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9991]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.21batch/s, loss=0.9991]\u001b[A\n",
      "Ep 2:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=0.9775]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.29batch/s, loss=0.9775]\u001b[A\n",
      "Ep 2:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.29batch/s, loss=0.9775]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.35batch/s, loss=0.9775]\u001b[A\n",
      "Ep 2:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.35batch/s, loss=0.9934]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.40batch/s, loss=0.9934]\u001b[A\n",
      "Ep 2:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=0.9818]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.43batch/s, loss=0.9818]\u001b[A\n",
      "Ep 2:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.43batch/s, loss=0.9898]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9898]\u001b[A\n",
      "Ep 2:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9927]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.47batch/s, loss=0.9927]\u001b[A\n",
      "Ep 2:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.47batch/s, loss=0.9666]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.48batch/s, loss=0.9666]\u001b[A\n",
      "Ep 2:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.48batch/s, loss=0.9622]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9622]\u001b[A\n",
      "Ep 2:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9892]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=0.9892]\u001b[A\n",
      "Ep 2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.9650]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.50batch/s, loss=0.9650]\u001b[A\n",
      "Ep 2:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.50batch/s, loss=0.9682]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9682]\u001b[A\n",
      "Ep 2:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.51batch/s, loss=0.9977]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.51batch/s, loss=0.9977]\u001b[A\n",
      "Ep 2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.51batch/s, loss=0.9610]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.51batch/s, loss=0.9610]\u001b[A\n",
      "Ep 2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.51batch/s, loss=0.9680]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9680]\u001b[A\n",
      "Ep 2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9630]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=0.9630]\u001b[A\n",
      "Ep 2:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=0.9696]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.51batch/s, loss=0.9696]\u001b[A\n",
      "Ep 2:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.51batch/s, loss=0.9823]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9823]\u001b[A\n",
      "Ep 2:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9526]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.9526]\u001b[A\n",
      "Ep 2:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9885]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=0.9885]\u001b[A\n",
      "Ep 2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=0.9636]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9636]\u001b[A\n",
      "Ep 2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9584]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9584]\u001b[A\n",
      "Ep 2:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9640]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9640]\u001b[A\n",
      "Ep 2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9643]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9643]\u001b[A\n",
      "Ep 2:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9504]\u001b[A\n",
      "Ep 2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9734]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9734]\u001b[A\n",
      "Ep 2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9473]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9473]\u001b[A\n",
      "Ep 2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9665]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9665]\u001b[A\n",
      "Ep 2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9889]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.51batch/s, loss=0.9889]\u001b[A\n",
      "Ep 2:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.51batch/s, loss=0.9692]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=0.9692]\u001b[A\n",
      "Ep 2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.51batch/s, loss=1.0010]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.51batch/s, loss=1.0010]\u001b[A\n",
      "Ep 2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.51batch/s, loss=0.9654]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9654]\u001b[A\n",
      "Ep 2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.51batch/s, loss=0.9367]\u001b[A\n",
      "Ep 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.76batch/s, loss=0.9367]\u001b[A\n",
      "Entrenando ghostnet:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:06<00:33, 33.28s/epoch, avg_loss=0.9738, val_acc=13.33%]\n",
      "Ep 3:   0%|          | 0/38 [00:00<?, ?batch/s]\u001b[A\n",
      "Ep 3:   0%|          | 0/38 [00:03<?, ?batch/s, loss=0.9629]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:03<02:13,  3.60s/batch, loss=0.9629]\u001b[A\n",
      "Ep 3:   3%|â–Ž         | 1/38 [00:04<02:13,  3.60s/batch, loss=0.9372]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:07,  1.87s/batch, loss=0.9372]\u001b[A\n",
      "Ep 3:   5%|â–Œ         | 2/38 [00:04<01:07,  1.87s/batch, loss=0.9660]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:04<00:46,  1.32s/batch, loss=0.9660]\u001b[A\n",
      "Ep 3:   8%|â–Š         | 3/38 [00:05<00:46,  1.32s/batch, loss=0.9765]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:05<00:35,  1.06s/batch, loss=0.9765]\u001b[A\n",
      "Ep 3:  11%|â–ˆ         | 4/38 [00:06<00:35,  1.06s/batch, loss=0.9592]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9592]\u001b[A\n",
      "Ep 3:  13%|â–ˆâ–Ž        | 5/38 [00:06<00:30,  1.09batch/s, loss=0.9847]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:06<00:26,  1.21batch/s, loss=0.9847]\u001b[A\n",
      "Ep 3:  16%|â–ˆâ–Œ        | 6/38 [00:07<00:26,  1.21batch/s, loss=0.9439]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:07<00:23,  1.29batch/s, loss=0.9439]\u001b[A\n",
      "Ep 3:  18%|â–ˆâ–Š        | 7/38 [00:08<00:23,  1.29batch/s, loss=0.9571]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=0.9571]\u001b[A\n",
      "Ep 3:  21%|â–ˆâ–ˆ        | 8/38 [00:08<00:22,  1.36batch/s, loss=0.9606]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:08<00:20,  1.40batch/s, loss=0.9606]\u001b[A\n",
      "Ep 3:  24%|â–ˆâ–ˆâ–Ž       | 9/38 [00:09<00:20,  1.40batch/s, loss=0.9669]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:09<00:19,  1.43batch/s, loss=0.9669]\u001b[A\n",
      "Ep 3:  26%|â–ˆâ–ˆâ–‹       | 10/38 [00:10<00:19,  1.43batch/s, loss=0.9563]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9563]\u001b[A\n",
      "Ep 3:  29%|â–ˆâ–ˆâ–‰       | 11/38 [00:10<00:18,  1.46batch/s, loss=0.9601]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:10<00:17,  1.47batch/s, loss=0.9601]\u001b[A\n",
      "Ep 3:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/38 [00:11<00:17,  1.47batch/s, loss=0.9310]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:11<00:16,  1.48batch/s, loss=0.9310]\u001b[A\n",
      "Ep 3:  34%|â–ˆâ–ˆâ–ˆâ–      | 13/38 [00:12<00:16,  1.48batch/s, loss=0.9729]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9729]\u001b[A\n",
      "Ep 3:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:12<00:16,  1.49batch/s, loss=0.9636]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:12<00:15,  1.50batch/s, loss=0.9636]\u001b[A\n",
      "Ep 3:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 15/38 [00:13<00:15,  1.50batch/s, loss=0.9524]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:13<00:14,  1.50batch/s, loss=0.9524]\u001b[A\n",
      "Ep 3:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 16/38 [00:14<00:14,  1.50batch/s, loss=0.9878]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.50batch/s, loss=0.9878]\u001b[A\n",
      "Ep 3:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/38 [00:14<00:13,  1.50batch/s, loss=0.9676]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:14<00:13,  1.50batch/s, loss=0.9676]\u001b[A\n",
      "Ep 3:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 18/38 [00:15<00:13,  1.50batch/s, loss=0.9635]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:15<00:12,  1.50batch/s, loss=0.9635]\u001b[A\n",
      "Ep 3:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 19/38 [00:16<00:12,  1.50batch/s, loss=0.9349]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9349]\u001b[A\n",
      "Ep 3:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 20/38 [00:16<00:11,  1.51batch/s, loss=0.9683]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:16<00:11,  1.51batch/s, loss=0.9683]\u001b[A\n",
      "Ep 3:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 21/38 [00:17<00:11,  1.51batch/s, loss=0.9792]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:17<00:10,  1.51batch/s, loss=0.9792]\u001b[A\n",
      "Ep 3:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 22/38 [00:18<00:10,  1.51batch/s, loss=0.9664]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9664]\u001b[A\n",
      "Ep 3:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 23/38 [00:18<00:09,  1.51batch/s, loss=0.9945]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:18<00:09,  1.51batch/s, loss=0.9945]\u001b[A\n",
      "Ep 3:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 24/38 [00:19<00:09,  1.51batch/s, loss=0.9594]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:19<00:08,  1.51batch/s, loss=0.9594]\u001b[A\n",
      "Ep 3:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 25/38 [00:20<00:08,  1.51batch/s, loss=0.9661]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9661]\u001b[A\n",
      "Ep 3:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 26/38 [00:20<00:07,  1.51batch/s, loss=0.9690]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:20<00:07,  1.51batch/s, loss=0.9690]\u001b[A\n",
      "Ep 3:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 27/38 [00:21<00:07,  1.51batch/s, loss=0.9513]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:21<00:06,  1.51batch/s, loss=0.9513]\u001b[A\n",
      "Ep 3:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 28/38 [00:22<00:06,  1.51batch/s, loss=0.9420]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9420]\u001b[A\n",
      "Ep 3:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 29/38 [00:22<00:05,  1.51batch/s, loss=0.9847]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:22<00:05,  1.51batch/s, loss=0.9847]\u001b[A\n",
      "Ep 3:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 30/38 [00:23<00:05,  1.51batch/s, loss=0.9405]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:23<00:04,  1.51batch/s, loss=0.9405]\u001b[A\n",
      "Ep 3:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/38 [00:24<00:04,  1.51batch/s, loss=0.9607]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9607]\u001b[A\n",
      "Ep 3:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/38 [00:24<00:03,  1.51batch/s, loss=0.9600]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:24<00:03,  1.51batch/s, loss=0.9600]\u001b[A\n",
      "Ep 3:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 33/38 [00:25<00:03,  1.51batch/s, loss=0.9458]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:25<00:02,  1.50batch/s, loss=0.9458]\u001b[A\n",
      "Ep 3:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 34/38 [00:26<00:02,  1.50batch/s, loss=1.0027]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.50batch/s, loss=1.0027]\u001b[A\n",
      "Ep 3:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/38 [00:26<00:01,  1.50batch/s, loss=0.9673]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:26<00:01,  1.50batch/s, loss=0.9673]\u001b[A\n",
      "Ep 3:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/38 [00:27<00:01,  1.50batch/s, loss=0.9278]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.50batch/s, loss=0.9278]\u001b[A\n",
      "Ep 3:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 37/38 [00:27<00:00,  1.50batch/s, loss=0.9618]\u001b[A\n",
      "Ep 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:27<00:00,  1.76batch/s, loss=0.9618]\u001b[A\n",
      "Entrenando ghostnet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:39<00:00, 33.28s/epoch, avg_loss=0.9619, val_acc=13.33%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§Š Optimizando (Poda + CuantizaciÃ³n)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_773191/2794115407.py:119: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  return torch.cat([x1, x2], dim=1)[:, :self.oup, :, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FINAL ghostnet: Acc=13.33% | Size=0.04 MB\n",
      "\n",
      "ðŸ† --- RESULTADOS FINALES ---\n",
      "Arquitectura    | Acc INT8   | Size (MB) \n",
      "---------------------------------------------\n",
      "repvgg          | 13.33%      | 0.23\n",
      "mobileone       | 13.33%      | 0.09\n",
      "ghostnet        | 13.33%      | 0.04\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "architectures = [\"repvgg\", \"mobileone\", \"ghostnet\"]\n",
    "\n",
    "# Con GPU, puedes subir los epochs a 10 o 20 sin esperar una eternidad\n",
    "for arch in architectures:\n",
    "    res = run_experiment(arch, train_loader, val_loader, epochs=3)\n",
    "    results.append(res)\n",
    "\n",
    "print(\"\\nðŸ† --- RESULTADOS FINALES ---\")\n",
    "print(f\"{'Arquitectura':<15} | {'Acc INT8':<10} | {'Size (MB)':<10}\")\n",
    "print(\"-\" * 45)\n",
    "for r in results:\n",
    "    print(f\"{r['arch']:<15} | {r['acc_int8']:.2f}%      | {r['size']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Audio Edge Research)",
   "language": "python",
   "name": "audio_edge_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
